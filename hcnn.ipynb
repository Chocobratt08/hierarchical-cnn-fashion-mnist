{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----data dir----\n",
    "data_dir = \"./data\"\n",
    "#-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "  learning_rate_init = 0.001\n",
    "  if epoch > 42:\n",
    "    learning_rate_init = 0.0002\n",
    "  if epoch > 52:\n",
    "    learning_rate_init = 0.00005\n",
    "  return learning_rate_init\n",
    "\n",
    "def unpickle(filename):\n",
    "  file = os.path.join(data_dir, filename)\n",
    "  with open(file, 'rb') as fo:\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "  return dict\n",
    "\n",
    "class LossWeightsModifier(keras.callbacks.Callback):\n",
    "  def __init__(self, alpha, beta, gamma):\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "    # customize your behavior\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch == 15:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.8)\n",
    "      K.set_value(self.gamma, 0.1)\n",
    "    if epoch == 25:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.2)\n",
    "      K.set_value(self.gamma, 0.7)\n",
    "    if epoch == 35:\n",
    "      K.set_value(self.alpha, 0)\n",
    "      K.set_value(self.beta, 0)\n",
    "      K.set_value(self.gamma, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- dimensions ---------\n",
    "height, width = 28, 28\n",
    "channel = 1\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (channel, height, width)\n",
    "else:\n",
    "    input_shape = (height, width, channel)\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 60000\n",
    "test_size = 10000\n",
    "\n",
    "#--- coarse 1 classes ---\n",
    "coarse1_classes = 2\n",
    "#--- coarse 2 classes ---\n",
    "coarse2_classes = 6\n",
    "#--- fine classes ---\n",
    "num_classes  = 10\n",
    "\n",
    "batch_size   = 128\n",
    "epochs       = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- file paths ---\n",
    "log_filepath = './tb_log_vgg16_hierarchy_dynamic/'\n",
    "weights_store_filepath = './vgg16_weights_hierarchy_dynamic/'\n",
    "retrain_id = '101'\n",
    "model_name = 'weights_vgg16_dynamic_cifar_100_'+retrain_id+'.h5'\n",
    "model_path = os.path.join(weights_store_filepath, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "class_names_c1 = ['Clothes', 'Goods']\n",
    "class_names_c2 = ['Tops', 'Bottoms', 'Dresses', 'Outers', 'Accessories', 'Shoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_to_c1 = {0:0, 1:0, 2:0, 3:0, 4:1, 5:1}\n",
    "fine_to_c2 = {0:0, 1:1, 2:0, 3:2, 4:3, 5:5, 6:0, 7:5, 8:4, 9:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mappings(mapping, source, dest):\n",
    "    for k,v in mapping.items():\n",
    "        print(source[k], \"->\", dest[v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tops -> Clothes\n",
      "Bottoms -> Clothes\n",
      "Dresses -> Clothes\n",
      "Outers -> Clothes\n",
      "Accessories -> Goods\n",
      "Shoes -> Goods\n",
      "----------\n",
      "T-shirt/top -> Tops\n",
      "Trouser -> Bottoms\n",
      "Pullover -> Tops\n",
      "Dress -> Dresses\n",
      "Coat -> Outers\n",
      "Sandal -> Shoes\n",
      "Shirt -> Tops\n",
      "Sneaker -> Shoes\n",
      "Bag -> Accessories\n",
      "Ankle boot -> Shoes\n"
     ]
    }
   ],
   "source": [
    "print_mappings(c2_to_c1, class_names_c2, class_names_c1)\n",
    "print(\"-\"*10)\n",
    "print_mappings(fine_to_c2, class_names, class_names_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_fine = to_categorical(train_labels)\n",
    "train_labels_fine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_fine = to_categorical(test_labels)\n",
    "test_labels_fine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_c2_index = [fine_to_c2[i] for i in train_labels]\n",
    "train_labels_c2 = to_categorical(train_labels_c2_index)\n",
    "train_labels_c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_c2_index = [fine_to_c2[i] for i in test_labels]\n",
    "test_labels_c2 = to_categorical(test_labels_c2_index)\n",
    "test_labels_c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_c1_index = [c2_to_c1[i] for i in train_labels_c2_index]\n",
    "train_labels_c1 = to_categorical(train_labels_c1_index)\n",
    "train_labels_c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_c1_index = [c2_to_c1[i] for i in test_labels_c2_index]\n",
    "test_labels_c1 = to_categorical(test_labels_c1_index)\n",
    "test_labels_c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[..., np.newaxis]\n",
    "x_test = test_images[..., np.newaxis]\n",
    "\n",
    "y_train = train_labels_fine\n",
    "y_test = test_labels_fine\n",
    "\n",
    "y_c1_train = train_labels_c1\n",
    "y_c1_test = test_labels_c1\n",
    "\n",
    "y_c2_train = train_labels_c2\n",
    "y_c2_test = test_labels_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28, 1)\n",
      "x_test shape:  (10000, 28, 28, 1)\n",
      "y_train shape:  (60000, 10)\n",
      "y_test shape:  (10000, 10)\n",
      "y_c1_train shape:  (60000, 2)\n",
      "y_c1_test shape:  (10000, 2)\n",
      "y_c2_train shape:  (60000, 6)\n",
      "y_c2_test shape:  (10000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"y_c1_train shape: \", y_c1_train.shape)\n",
    "print(\"y_c1_test shape: \", y_c1_test.shape)\n",
    "print(\"y_c2_train shape: \", y_c2_train.shape)\n",
    "print(\"y_c2_test shape: \", y_c2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(None, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "img_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='vgg16_hierarchy')\n",
    "#model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "19328/60000 [========>.....................] - ETA: 42:27 - loss: 0.1619 - c1_predictions_cifar10_loss: 0.1096 - c2_predictions_cifar100_loss: 2.2076 - predictions_cifar100_loss: 3.2367 - c1_predictions_cifar10_accuracy: 0.9600 - c2_predictions_cifar100_accuracy: 0.3223 - predictions_cifar100_accuracy: 0.1633"
     ]
    }
   ],
   "source": [
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "cbks = [change_lr, tb_cb, change_lw]\n",
    "\n",
    "model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
